{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small and Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "img_width, img_height = 256, 256\n",
    "train_data_dir = \"data/train\"\n",
    "validation_data_dir = \"data/test\"\n",
    "nb_train_samples = 28709\n",
    "nb_validation_samples = 7178 \n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "\n",
    "model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:64: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:64: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., steps_per_epoch=1794, validation_data=<keras.pre..., epochs=5, callbacks=[<keras.ca..., validation_steps=7178)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.8249 - acc: 0.2373"
     ]
    }
   ],
   "source": [
    "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
    "# for layer in model.layers[:5]:\n",
    "#     layer.trainable = False\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "#Adding custom Layers \n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(7, activation=\"softmax\")(x)\n",
    "\n",
    "# creating the final model \n",
    "model_final = Model(input = model.input, output = predictions)\n",
    "\n",
    "# compile the model \n",
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size, \n",
    "class_mode = \"categorical\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "validation_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "class_mode = \"categorical\")\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "samples_per_epoch = nb_train_samples,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "nb_val_samples = nb_validation_samples,\n",
    "callbacks = [checkpoint, early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small and Different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "img_width, img_height = 256, 256\n",
    "\n",
    "### Build the network \n",
    "img_input = Input(shape=(256, 256, 3))\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "# Block 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "model = Model(input = img_input, output = x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "[layer.name for layer in model.layers]\n",
    "\"\"\"\n",
    "['input_1',\n",
    " 'block1_conv1',\n",
    " 'block1_conv2',\n",
    " 'block1_pool',\n",
    " 'block2_conv1',\n",
    " 'block2_conv2',\n",
    " 'block2_pool']\n",
    "\"\"\"\n",
    "\n",
    "import h5py\n",
    "weights_path = 'vgg19_weights.h5' # ('https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5)\n",
    "f = h5py.File(weights_path)\n",
    "\n",
    "list(f[\"model_weights\"].keys())\n",
    "\"\"\"\n",
    "['block1_conv1',\n",
    " 'block1_conv2',\n",
    " 'block1_pool',\n",
    " 'block2_conv1',\n",
    " 'block2_conv2',\n",
    " 'block2_pool',\n",
    " 'block3_conv1',\n",
    " 'block3_conv2',\n",
    " 'block3_conv3',\n",
    " 'block3_conv4',\n",
    " 'block3_pool',\n",
    " 'block4_conv1',\n",
    " 'block4_conv2',\n",
    " 'block4_conv3',\n",
    " 'block4_conv4',\n",
    " 'block4_pool',\n",
    " 'block5_conv1',\n",
    " 'block5_conv2',\n",
    " 'block5_conv3',\n",
    " 'block5_conv4',\n",
    " 'block5_pool',\n",
    " 'dense_1',\n",
    " 'dense_2',\n",
    " 'dense_3',\n",
    " 'dropout_1',\n",
    " 'global_average_pooling2d_1',\n",
    " 'input_1']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list all the layer names which are in the model.\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "\"\"\"\n",
    "# Here we are extracting model_weights for each and every layer from the .h5 file\n",
    "f[\"model_weights\"][\"block1_conv1\"].attrs[\"weight_names\"]\n",
    "array([b'block1_conv1/kernel:0', b'block1_conv1/bias:0'], \n",
    "      dtype='|S21')\n",
    "# we are assiging this array to weight_names below \n",
    ">>> f[\"model_weights\"][\"block1_conv1\"][\"block1_conv1/kernel:0]\n",
    "<HDF5 dataset \"kernel:0\": shape (3, 3, 3, 64), type \"<f4\">\n",
    "# The list comprehension (weights) stores these two weights and bias of both the layers \n",
    ">>>layer_names.index(\"block1_conv1\")\n",
    "1\n",
    ">>> model.layers[1].set_weights(weights)\n",
    "# This will set the weights for that particular layer.\n",
    "With a for loop we can set_weights for the entire network.\n",
    "\"\"\"\n",
    "for i in layer_dict.keys():\n",
    "    weight_names = f[\"model_weights\"][i].attrs[\"weight_names\"]\n",
    "    weights = [f[\"model_weights\"][i][j] for j in weight_names]\n",
    "    index = layer_names.index(i)\n",
    "    model.layers[index].set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "features = []\n",
    "for i in tqdm(files_location):\n",
    "        im = cv2.imread(i)\n",
    "        im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (256, 256)).astype(np.float32) / 255.0\n",
    "        im = np.expand_dims(im, axis =0)\n",
    "        outcome = model_final.predict(im)\n",
    "        features.append(outcome)\n",
    "        \n",
    "## collect these features and create a dataframe and train a classfier on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
